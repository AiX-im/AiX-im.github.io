---
title:          "NeutronAscend: Optimizing GNN Training with Ascend AI Processors"
date:           2025-08-26 00:01:00 +0800
selected:       false
pub:            "Transactions on Architecture and Code Optimization (TACO)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"
abstract: >-
  The Ascend AI processor is typically architected on multiple AI Cores that are physically decoupled and designed for dense matrix computation. When processing graph data with inherent sparsity and power-law distribution, the Ascend AI processors suffer from the inter-core workload imbalance and inefficient intra-core resource utilization. In this paper, we present NeutronAscend, an efficient GNN training framework tailored for the Ascend AI processor. NeutronAscend employs two critical designs for both inter-core and intra-core performance optimization. 
cover:          /assets/images/covers/taco-neutronascend.png
authors:
  - "<b style='font-weight:900;color:#000;'>Xin Ai</b>"
  - Bing Zhang
  - Qiange Wang
  - Yanfeng Zhang
  - Hao Yuan
  - Shufeng Gong
  - Ge Yu
links:
  Paper: https://dl.acm.org/doi/pdf/10.1145/3762662
  # Code: https://github.com/iDC-NEU/NeutronRAG
  # Video: https://www.bilibili.com/video/BV1KcVuzDEdF/?vd_source=2cab48c20a54d99e4f559a22c60e7fc3
---
